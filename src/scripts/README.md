# Scripts - MÃ³dulos de Processamento de Dados

Este diretÃ³rio contÃ©m os mÃ³dulos responsÃ¡veis pela pipeline de ETL, anÃ¡lises estatÃ­sticas e utilitÃ¡rios para consumo de dados por modelos de ML.

---

## Estrutura de Arquivos

### **Core Pipeline**

#### `data_processing_pipeline.py`
Pipeline principal que orquestra a execuÃ§Ã£o completa do processamento de dados.
- Executa limpeza de dados (data cleaning)
- Executa feature engineering
- Gera logs e estatÃ­sticas de execuÃ§Ã£o
- Pode ser executado diretamente: `python -m src.scripts.data_processing_pipeline`

#### `data_cleaning.py`
MÃ³dulo responsÃ¡vel pela limpeza e validaÃ§Ã£o dos dados brutos.
- Remove/trata valores nulos e duplicatas
- Normaliza categorias problemÃ¡ticas
- Cria IDs Ãºnicos
- Transforma colunas (ex: availability yes/no â†’ 1/0)
- **Entrada:** `data/raw/all_books_with_images.csv`
- **SaÃ­da:** `data/processed/books_processed.csv`

#### `feature_engineering.py`
Cria features derivadas para anÃ¡lise e modelos de ML.
- CategorizaÃ§Ã£o de preÃ§os (price_range)
- Features de tÃ­tulo (has_subtitle, has_series, title_length, etc.)
- CategorizaÃ§Ã£o de ratings e stock
- One-hot encoding de categorias
- Score de popularidade
- **Entrada:** `data/processed/books_processed.csv`
- **SaÃ­da:** `data/features/books_features.csv`

---

### **Data Types & Configuration**

#### `data_types.py`
Define modelos Pydantic e tipos de dados usados em toda a aplicaÃ§Ã£o.
- `PipelineConfig`: ConfiguraÃ§Ã£o da pipeline (caminhos, parÃ¢metros)
- `PipelineStats`: EstatÃ­sticas de execuÃ§Ã£o
- Enums: `PriceRange`, `RatingCategory`, `StockLevel`
- Garante validaÃ§Ã£o e type safety

---

### **Statistics & ML Data**

#### `book_statistics.py`
FunÃ§Ãµes para cÃ¡lculo de estatÃ­sticas sobre a coleÃ§Ã£o de livros.
- `get_overview_statistics()`: Total de livros, preÃ§o mÃ©dio, distribuiÃ§Ã£o de ratings
- `get_category_statistics()`: MÃ©tricas agregadas por categoria
- `get_top_rated_books()`: Livros com melhores avaliaÃ§Ãµes
- `get_books_in_price_range()`: Filtro por faixa de preÃ§o
- Usado pelos endpoints `/api/v1/stats/*` e `/api/v1/books/*`

#### `ml_data.py`
UtilitÃ¡rios para preparaÃ§Ã£o de dados voltados a modelos de ML.
- Carregamento com cache do dataset de features
- `get_features_dataframe()`: Retorna features completas
- `get_training_split()`: DivisÃ£o treino/teste (70/30 padrÃ£o)
- Casting e normalizaÃ§Ã£o de tipos
- Usado pelos endpoints `/api/v1/ml/*`

#### `ml_datasets.py`
PreparaÃ§Ã£o de datasets especÃ­ficos para treinamento/inferÃªncia.
- `get_feature_matrix()`: Matriz de features para modelos
- `get_training_dataset()`: Dataset formatado para treino/teste
- Suporta seleÃ§Ã£o de colunas e splits customizados

---

### **Web Scraping**

#### `scrapper_lib.py`
Biblioteca para extraÃ§Ã£o de dados de livros da web.
- `trigger_scrap()`: Inicia processo de scraping
- Coleta tÃ­tulos, preÃ§os, ratings, categorias, imagens
- Salva dados brutos em `data/raw/all_books_with_images.csv`
- Usado pelo endpoint `/scrapper`

---

## Uso RÃ¡pido

### Executar Pipeline Completa
```bash
python -m src.scripts.data_processing_pipeline
```

### Importar em CÃ³digo
```python
from src.scripts import run_pipeline
from src.scripts.book_statistics import get_overview_statistics
from src.scripts.ml_data import get_training_split

# Executar pipeline
stats = run_pipeline()

# Obter estatÃ­sticas
overview = get_overview_statistics()

# Preparar dados para ML
train_df, test_df = get_training_split(ratio=0.7)
```

---

## Fluxo de Dados

```
[Web Scraping]
    â†“
data/raw/all_books_with_images.csv
    â†“
[Data Cleaning]
    â†“
data/processed/books_processed.csv
    â†“
[Feature Engineering]
    â†“
data/features/books_features.csv
    â†“
[Statistics & ML APIs]
```

---

## ConfiguraÃ§Ã£o

A pipeline usa `PipelineConfig` definido em `data_types.py`:

```python
PipelineConfig(
    input_file="data/raw/all_books_with_images.csv",
    processed_output="data/processed/books_processed.csv",
    features_output="data/features/books_features.csv",
    default_category="Outros",
    problematic_categories=["Add a comment", "Default"]
)
```

## ğŸ“Š Resultados da Ãšltima ExecuÃ§Ã£o

- **Registros processados:** 1000
- **Tempo de execuÃ§Ã£o:** 0.42s  
- **Features criadas:** 59
- **Categorias limpas:** 219
- **DistribuiÃ§Ã£o de preÃ§os:**
  - MÃ©dio (20-40): 401 livros (40.1%)
  - Alto (40-50): 205 livros (20.5%)
  - Premium (>50): 198 livros (19.8%)
  - Baixo (â‰¤20): 196 livros (19.6%)

## ğŸ”§ DependÃªncias

- `polars` - Processamento eficiente de dados
- `pydantic` - ValidaÃ§Ã£o e tipagem de dados
- `pathlib` - ManipulaÃ§Ã£o de caminhos
- `uuid` - GeraÃ§Ã£o de IDs Ãºnicos

## ğŸ“ˆ PrÃ³ximos Passos

Os dados estÃ£o prontos para:
- âœ… AnÃ¡lise exploratÃ³ria avanÃ§ada
- âœ… Modelagem preditiva de preÃ§os
- âœ… Sistema de recomendaÃ§Ã£o de livros
- âœ… API de consulta de dados

## ğŸ¯ Qualidade dos Dados

- **0 valores nulos** - Dataset Ã­ntegro
- **1000 IDs Ãºnicos** - IdentificaÃ§Ã£o consistente
- **59 features** - Rico conjunto para ML
- **ValidaÃ§Ã£o completa** - Dados confiÃ¡veis para anÃ¡lise

---

**Desenvolvido para Tech Challenge**  
*Pipeline otimizada com Polars + Pydantic*